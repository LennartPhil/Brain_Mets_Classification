{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2D CNN training\n",
    "This notebook works as a proof of concept version for the actual script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pretrained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vision Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the standard libraries, watchout though: vit_keras uses tensorflow_addons, which has stopped any development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import random\n",
    "import os\n",
    "import glob\n",
    "from functools import partial\n",
    "from time import strftime\n",
    "\n",
    "from vit_keras import vit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "helper functions below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_tfrs = \"/Users/LennartPhilipp/Desktop/Uni/Prowiss/Datensatz_RGB/regensburg_slices_tfrecords/all_pats_single_rgb\"\n",
    "path_to_logs = \"/Users/LennartPhilipp/Desktop/Uni/Prowiss/Datensatz_RGB/regensburg_slices_tfrecords/test_logs\"\n",
    "\n",
    "training_codename = \"2D_pretrained_0000\"\n",
    "\n",
    "num_classes = 3\n",
    "sequence_to_train_on = \"t1c\"\n",
    "\n",
    "time = strftime(\"run_%Y_%m_%d_%H_%M_%S\")\n",
    "class_directory = f\"{training_codename}_{num_classes}_classes_{time}\"\n",
    "\n",
    "path_to_callbacks = Path(path_to_logs) / Path(class_directory)\n",
    "os.makedirs(path_to_callbacks, exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "image_size = 224\n",
    "\n",
    "## train / val / test split\n",
    "train_ratio = 0.8\n",
    "val_ratio = 0.1\n",
    "test_ratio = 0.1\n",
    "\n",
    "early_stopping_patience = 150\n",
    "shuffle_buffer_size = 20\n",
    "repeat_count = 1\n",
    "\n",
    "image_size = 224\n",
    "batch_size = 16\n",
    "EPOCHS = 30\n",
    "\n",
    "len_train = 0\n",
    "len_val = 0\n",
    "len_test = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_patient_paths():\n",
    "    patients = [f for f in os.listdir(path_to_tfrs) if os.path.isdir(os.path.join(path_to_tfrs, f))]\n",
    "\n",
    "    patient_paths = [str(path_to_tfrs) + \"/\" + patient for patient in patients]\n",
    "\n",
    "    print(f\"total patients: {len(patient_paths)}\")\n",
    "\n",
    "    for path in patient_paths:\n",
    "        patient_not_empty = False\n",
    "        patient_files = os.listdir(path)\n",
    "        for file in patient_files:\n",
    "            if file.endswith(\".tfrecord\"):\n",
    "                patient_not_empty = True\n",
    "        \n",
    "        if patient_not_empty == False:\n",
    "            patient_paths.remove(path)\n",
    "\n",
    "    return patient_paths\n",
    "\n",
    "def split_patients(patient_paths, fraction_to_use = 1):\n",
    "\n",
    "    random.shuffle(patient_paths)\n",
    "\n",
    "    patient_paths = patient_paths[:int(len(patient_paths) * fraction_to_use)]\n",
    "\n",
    "    if fraction_to_use != 1:\n",
    "        print(f\"actual tfrs length: {len(patient_paths)}\")\n",
    "\n",
    "    train_size = int(len(patient_paths) * train_ratio)\n",
    "    val_size = int(len(patient_paths) * val_ratio)\n",
    "\n",
    "    train_patients_paths = patient_paths[:train_size]\n",
    "    val_patients_paths = patient_paths[train_size:train_size + val_size]\n",
    "    test_patients_paths = patient_paths[train_size + val_size:]\n",
    "\n",
    "    print(f\"train: {len(train_patients_paths)} | val: {len(val_patients_paths)} | test: {len(test_patients_paths)}\")\n",
    "\n",
    "    # save train / val / test patients to txt file\n",
    "    # hf.save_paths_to_txt(train_patients_paths, \"train\", path_to_callbacks)\n",
    "    # hf.save_paths_to_txt(val_patients_paths, \"val\", path_to_callbacks)\n",
    "    # hf.save_paths_to_txt(test_patients_paths, \"test\", path_to_callbacks)\n",
    "\n",
    "    sum = len(train_patients_paths) + len(val_patients_paths) + len(test_patients_paths)\n",
    "    if sum != len(patient_paths):\n",
    "        print(\"WARNING: error occured in train / val / test split!\")\n",
    "\n",
    "    return train_patients_paths, val_patients_paths, test_patients_paths\n",
    "\n",
    "def get_tfr_paths_for_patients(patient_paths):\n",
    "\n",
    "    tfr_paths = []\n",
    "\n",
    "    for patient in patient_paths:\n",
    "        tfr_paths.extend(glob.glob(patient + \"/*.tfrecord\"))\n",
    "    \n",
    "    for path in tfr_paths:\n",
    "        verify_tfrecord(path)\n",
    "\n",
    "    #print(f\"total tfrs: {len(tfr_paths)}\")\n",
    "\n",
    "    return tfr_paths\n",
    "\n",
    "def read_data(train_paths, val_paths, test_paths = None):\n",
    "\n",
    "    train_data = tf.data.Dataset.from_tensor_slices(train_paths)\n",
    "    val_data = tf.data.Dataset.from_tensor_slices(val_paths)\n",
    "\n",
    "    train_data = train_data.interleave(\n",
    "        lambda x: tf.data.TFRecordDataset([x], compression_type=\"GZIP\"),\n",
    "        num_parallel_calls=tf.data.AUTOTUNE,\n",
    "        deterministic=False\n",
    "    )\n",
    "    val_data = val_data.interleave(\n",
    "        lambda x: tf.data.TFRecordDataset([x], compression_type=\"GZIP\"),\n",
    "        num_parallel_calls=tf.data.AUTOTUNE,\n",
    "        deterministic=False\n",
    "    )\n",
    "\n",
    "    train_data = train_data.map(partial(parse_record, image_only = True, labeled = True, num_classes = num_classes), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    val_data = val_data.map(partial(parse_record, image_only = True, labeled = True, num_classes = num_classes), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "    train_data = train_data.shuffle(buffer_size=shuffle_buffer_size)\n",
    "    val_data = val_data.shuffle(buffer_size=shuffle_buffer_size)\n",
    "\n",
    "    train_data = train_data.repeat(count = repeat_count)\n",
    "    val_data = val_data.repeat(count = repeat_count)\n",
    "\n",
    "    train_data = train_data.batch(batch_size)\n",
    "    val_data = val_data.batch(batch_size)\n",
    "\n",
    "    train_data = train_data.prefetch(buffer_size=1)\n",
    "    val_data = val_data.prefetch(buffer_size=1)\n",
    "\n",
    "    if test_paths is not None:\n",
    "        test_data = tf.data.Dataset.from_tensor_slices(test_paths)\n",
    "        test_data = test_data.interleave(\n",
    "            lambda x: tf.data.TFRecordDataset([x], compression_type=\"GZIP\"),\n",
    "            num_parallel_calls=tf.data.AUTOTUNE,\n",
    "            deterministic=False\n",
    "        )\n",
    "        test_data = test_data.map(partial(parse_record, image_only = True, labeled = True, num_classes = num_classes), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        test_data = test_data.batch(batch_size)\n",
    "        test_data = test_data.prefetch(buffer_size=1)\n",
    "\n",
    "        return train_data, val_data, test_data\n",
    "\n",
    "    return train_data, val_data\n",
    "\n",
    "def parse_record(record, image_only = False, labeled = False, num_classes = 2, sequence = \"t1c\"):\n",
    "\n",
    "    feature_description = {\n",
    "        \"image\": tf.io.FixedLenFeature([240, 240, 3, 4], tf.float32),\n",
    "        \"sex\": tf.io.FixedLenFeature([], tf.int64, default_value=[0]),\n",
    "        \"age\": tf.io.FixedLenFeature([], tf.int64, default_value=0),\n",
    "        \"primary\": tf.io.FixedLenFeature([], tf.int64, default_value=0),\n",
    "    }\n",
    "\n",
    "    example = tf.io.parse_single_example(record, feature_description)\n",
    "    image = example[\"image\"]\n",
    "    image = tf.reshape(image, [240, 240, 3, 4])\n",
    "    #image = data_augmentation(image)\n",
    "\n",
    "    # primary should have a value between 0 and 5\n",
    "    # depending on num classes return different values\n",
    "    # if num_classes = 2, return 1 if primary is 1, else 0\n",
    "    # if num_classes = 3, return primaries 1 and 2, else 0\n",
    "    # if num_classes = 4, return primaries 1, 2 and 3, else 0\n",
    "    # if num_classes = 5, return primaries 1, 2, 3 and 4, else 0\n",
    "    # if num_classes = 6, return primaries 1, 2, 3, 4 and 5, else 0\n",
    "\n",
    "    primary_to_return = tf.constant(0, dtype=tf.int64)\n",
    "\n",
    "    if num_classes == 2:\n",
    "        if example[\"primary\"] == tf.constant(1, dtype=tf.int64):\n",
    "            primary_to_return = example[\"primary\"]\n",
    "        else:\n",
    "            primary_to_return = tf.constant(0, dtype=tf.int64)\n",
    "    elif num_classes == 3:\n",
    "        if example[\"primary\"] == tf.constant(1, dtype=tf.int64) or example[\"primary\"] == tf.constant(2, dtype=tf.int64):\n",
    "            primary_to_return = example[\"primary\"]\n",
    "        else:\n",
    "            primary_to_return = tf.constant(0, dtype=tf.int64)\n",
    "    elif num_classes == 4:\n",
    "        if example[\"primary\"] == tf.constant(1, dtype=tf.int64) or example[\"primary\"] == tf.constant(2, dtype=tf.int64) or example[\"primary\"] == tf.constant(3, dtype=tf.int64):\n",
    "            primary_to_return = example[\"primary\"]\n",
    "        else:\n",
    "            primary_to_return = tf.constant(0, dtype=tf.int64)\n",
    "    elif num_classes == 5:\n",
    "        if example[\"primary\"] == tf.constant(1, dtype=tf.int64) or example[\"primary\"] == tf.constant(2, dtype=tf.int64) or example[\"primary\"] == tf.constant(3, dtype=tf.int64) or example[\"primary\"] == tf.constant(4, dtype=tf.int64):\n",
    "            primary_to_return = example[\"primary\"]\n",
    "        else:\n",
    "            primary_to_return = tf.constant(0, dtype=tf.int64)\n",
    "    elif num_classes == 6:\n",
    "        if example[\"primary\"] == tf.constant(1, dtype=tf.int64) or example[\"primary\"] == tf.constant(2, dtype=tf.int64) or example[\"primary\"] == tf.constant(3, dtype=tf.int64) or example[\"primary\"] == tf.constant(4, dtype=tf.int64) or example[\"primary\"] == tf.constant(5, dtype=tf.int64):\n",
    "            primary_to_return = example[\"primary\"]\n",
    "        else:\n",
    "            primary_to_return = tf.constant(0, dtype=tf.int64)\n",
    "    else:\n",
    "            print(\"ERROR\")\n",
    "            print(\"num classes not supported\")\n",
    "            print(\"Check parse_record function\")\n",
    "            print(\"____________________________\")\n",
    "\n",
    "    \n",
    "    if sequence == \"t1\":\n",
    "        image = image[:, :, :, 0]\n",
    "    elif sequence == \"t1c\":\n",
    "        image = image[:, :, :, 1]\n",
    "    elif sequence == \"t2\":\n",
    "        image = image[:, :, :, 2]\n",
    "    elif sequence == \"flair\":\n",
    "        image = image[:, :, :, 3]\n",
    "\n",
    "    if image_only:\n",
    "        return image, primary_to_return\n",
    "    elif labeled:\n",
    "        return (image, example[\"sex\"], example[\"age\"]), primary_to_return #example[\"primary\"]\n",
    "    else:\n",
    "        return image\n",
    "    \n",
    "def verify_tfrecord(file_path):\n",
    "    try:\n",
    "        for _ in tf.data.TFRecordDataset(file_path, compression_type=\"GZIP\"):\n",
    "            pass\n",
    "    except tf.errors.DataLossError:\n",
    "        print(f\"Corrupted TFRecord file: {file_path}\")\n",
    "\n",
    "def get_callbacks(fold_num = 0,\n",
    "                  use_checkpoint = True,\n",
    "                  use_early_stopping = True,\n",
    "                  early_stopping_patience = early_stopping_patience,\n",
    "                  use_tensorboard = True,\n",
    "                  use_csv_logger = True,\n",
    "                  use_lrscheduler = False):\n",
    "\n",
    "    callbacks = []\n",
    "\n",
    "    path_to_fold_callbacks = path_to_callbacks / f\"fold_{fold_num}\"\n",
    "\n",
    "    def get_run_logdir(root_logdir = path_to_fold_callbacks / \"tensorboard\"):\n",
    "        return Path(root_logdir) / strftime(\"run_%Y_%m_%d_%H_%M_%S\")\n",
    "\n",
    "    run_logdir = get_run_logdir()\n",
    "\n",
    "    # model checkpoint\n",
    "    if use_checkpoint:\n",
    "        checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n",
    "            filepath = path_to_fold_callbacks / \"saved_weights.weights.h5\",\n",
    "            monitor = \"val_accuracy\",\n",
    "            mode = \"max\",\n",
    "            save_best_only = True,\n",
    "            save_weights_only = True,\n",
    "        )\n",
    "        callbacks.append(checkpoint_cb)\n",
    "\n",
    "    # early stopping\n",
    "    if use_early_stopping:\n",
    "        early_stopping_cb = tf.keras.callbacks.EarlyStopping(\n",
    "            patience = early_stopping_patience,\n",
    "            restore_best_weights = True,\n",
    "            verbose = 1\n",
    "        )\n",
    "        callbacks.append(early_stopping_cb)\n",
    "\n",
    "    # tensorboard, doesn't really work yet\n",
    "    if use_tensorboard:\n",
    "        tensorboard_cb = tf.keras.callbacks.TensorBoard(log_dir = run_logdir,\n",
    "                                                    histogram_freq = 1)\n",
    "        callbacks.append(tensorboard_cb)\n",
    "    \n",
    "    # csv logger\n",
    "    if use_csv_logger:\n",
    "        csv_logger_cb = tf.keras.callbacks.CSVLogger(path_to_fold_callbacks / \"training.csv\", separator = \",\", append = True)\n",
    "        callbacks.append(csv_logger_cb)\n",
    "    \n",
    "    if use_lrscheduler:\n",
    "        lr_schedule = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-8 * 10**(epoch * 0.0175))\n",
    "        callbacks.append(lr_schedule)\n",
    "\n",
    "    print(\"get_callbacks successful\")\n",
    "\n",
    "    return callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalizeToRange(tf.keras.layers.Layer):\n",
    "    def __init__(self, zero_to_one=True):\n",
    "        super(NormalizeToRange, self).__init__()\n",
    "        self.zero_to_one = zero_to_one\n",
    "\n",
    "    def call(self, inputs):\n",
    "        min_val = tf.reduce_min(inputs)\n",
    "        max_val = tf.reduce_max(inputs)\n",
    "        if self.zero_to_one:\n",
    "            # Normalize to [0, 1]\n",
    "            normalized = (inputs - min_val) / (max_val - min_val)\n",
    "        else:\n",
    "            # Normalize to [-1, 1]\n",
    "            normalized = 2 * (inputs - min_val) / (max_val - min_val) - 1\n",
    "        return normalized\n",
    "    \n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    tf.keras.layers.RandomFlip(mode = \"horizontal\"),\n",
    "    #tf.keras.layers.Rescaling(1/255),\n",
    "    tf.keras.layers.RandomContrast(0.5), # consider removing the random contrast layer as that causes pixel values to go beyond 1\n",
    "    tf.keras.layers.RandomBrightness(factor = (-0.2, 0.4)), #, value_range=(0, 1)\n",
    "    tf.keras.layers.RandomRotation(factor = (-0.1, 0.1), fill_mode = \"nearest\"),\n",
    "    NormalizeToRange(zero_to_one=True),\n",
    "    tf.keras.layers.RandomTranslation(\n",
    "        height_factor = 0.05,\n",
    "        width_factor = 0.05,\n",
    "        fill_mode = \"nearest\",\n",
    "        interpolation = \"bilinear\"\n",
    "    ),\n",
    "    tf.keras.layers.Resizing(image_size, image_size)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total patients: 276\n",
      "train: 216 | val: 27 | test: 28\n",
      "train: 926 | val: 80 | test: 72\n",
      "get_callbacks successful\n"
     ]
    }
   ],
   "source": [
    "patients = get_patient_paths()\n",
    "\n",
    "train_patients, val_patients, test_patients = split_patients(patients, fraction_to_use = 1)\n",
    "\n",
    "train_paths = get_tfr_paths_for_patients(train_patients)\n",
    "val_paths = get_tfr_paths_for_patients(val_patients)\n",
    "test_paths = get_tfr_paths_for_patients(test_patients)\n",
    "train_data, val_data, test_data = read_data(train_paths, val_paths, test_paths)\n",
    "\n",
    "len_train = len(train_paths)\n",
    "len_val = len(val_paths)\n",
    "len_test = len(test_paths)\n",
    "print(f\"train: {len_train} | val: {len_val} | test: {len_test}\")\n",
    "\n",
    "callbacks = get_callbacks(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "(16, 240, 240, 3)\n",
      "(240, 240, 3)\n",
      "Min: 0.0\n",
      "Max: 255.0\n",
      "Mean: 0.9764930605888367\n",
      "\n",
      "Min: 0.0\n",
      "Max: 1.0\n",
      "Mean: 0.004145030397921801\n",
      "(224, 224, 3)\n",
      "\n",
      "(16,)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_image = train_data.take(1)\n",
    "\n",
    "for image, primary in test_image:\n",
    "    print(primary.numpy()[0])\n",
    "    print(image.shape)\n",
    "    print(image[0].shape)\n",
    "    image = image[0]\n",
    "    print(f\"Min: {tf.reduce_min(image)}\")\n",
    "    print(f\"Max: {tf.reduce_max(image)}\")\n",
    "    print(f\"Mean: {tf.reduce_mean(image)}\")\n",
    "\n",
    "    image = data_augmentation(image)\n",
    "    print()\n",
    "    print(f\"Min: {tf.reduce_min(image)}\")\n",
    "    print(f\"Max: {tf.reduce_max(image)}\")\n",
    "    print(f\"Mean: {tf.reduce_mean(image)}\")\n",
    "    print(image.shape)\n",
    "\n",
    "    print()\n",
    "    print(primary.numpy().shape)\n",
    "\n",
    "    plt.imshow(image, cmap = \"gray\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/LennartPhilipp/Desktop/Uni/Prowiss/Code/Brain_Mets_Classification/brain_mets_env/lib/python3.11/site-packages/vit_keras/utils.py:81: UserWarning: Resizing position embeddings from 24, 24 to 14, 14\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "vit_model = vit.vit_b16(\n",
    "        image_size = image_size,\n",
    "        activation = 'softmax',\n",
    "        pretrained = True,\n",
    "        include_top = False,\n",
    "        pretrained_top = False,\n",
    "        #classes = 3\n",
    "        )\n",
    "\n",
    "#vit_model.build(input_shape=(image_size, image_size, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Patches(tf.keras.layers.Layer):\n",
    "    def __init__(self, patch_size):\n",
    "        super(Patches, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def call(self, images):\n",
    "        batch_size = tf.shape(images)[0]\n",
    "        patches = tf.image.extract_patches(\n",
    "            images = images,\n",
    "            sizes = [1, self.patch_size, self.patch_size, 1],\n",
    "            strides = [1, self.patch_size, self.patch_size, 1],\n",
    "            rates = [1, 1, 1, 1],\n",
    "            padding = 'VALID',\n",
    "        )\n",
    "        patch_dims = patches.shape[-1]\n",
    "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
    "        return patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vision_transformer\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " sequential_27 (Sequential)  (224, 224, 3)             0         \n",
      "                                                                 \n",
      " vit-b16 (Functional)        (None, 768)               85798656  \n",
      "                                                                 \n",
      " flatten_13 (Flatten)        (None, 768)               0         \n",
      "                                                                 \n",
      " batch_normalization_26 (Ba  (None, 768)               3072      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_52 (Dense)            (None, 128)               98432     \n",
      "                                                                 \n",
      " batch_normalization_27 (Ba  (None, 128)               512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_53 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_54 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_55 (Dense)            (None, 3)                 99        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 85911107 (327.72 MB)\n",
      "Trainable params: 85909315 (327.72 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#input_shape = (224, 224, 3)\n",
    "#inputs = tf.keras.Input(shape = input_shape)\n",
    "\n",
    "model_vit = tf.keras.Sequential([\n",
    "        tf.keras.layers.InputLayer(input_shape = (240, 240, 3)),\n",
    "        data_augmentation,\n",
    "        vit_model,\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dense(128, activation = tf.keras.activations.gelu),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dense(64, activation = tf.keras.activations.gelu),\n",
    "        tf.keras.layers.Dense(32, activation = tf.keras.activations.gelu),\n",
    "        tf.keras.layers.Dense(3, activation = 'softmax')\n",
    "    ],\n",
    "    name = 'vision_transformer')\n",
    "\n",
    "model_vit.build()\n",
    "\n",
    "model_vit.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49/49 [==============================] - 186s 4s/step - loss: 1.1954 - accuracy: 0.3222 - val_loss: 1.5543 - val_accuracy: 0.2365\n",
      "Epoch 2/30\n",
      "49/49 [==============================] - 176s 4s/step - loss: 1.0872 - accuracy: 0.4433 - val_loss: 2.0845 - val_accuracy: 0.2500\n",
      "Epoch 3/30\n",
      "49/49 [==============================] - 180s 4s/step - loss: 1.0691 - accuracy: 0.4845 - val_loss: 1.8574 - val_accuracy: 0.1757\n",
      "Epoch 4/30\n",
      "49/49 [==============================] - 180s 4s/step - loss: 1.0676 - accuracy: 0.4601 - val_loss: 1.7427 - val_accuracy: 0.2838\n",
      "Epoch 5/30\n",
      "34/49 [===================>..........] - ETA: 51s - loss: 1.0236 - accuracy: 0.5129"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[353], line 12\u001b[0m\n\u001b[1;32m      5\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer \u001b[38;5;241m=\u001b[39m optimizer, \n\u001b[1;32m      6\u001b[0m               loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse_categorical_crossentropy\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m      7\u001b[0m               metrics \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     10\u001b[0m early_stopping_callbacks \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mEarlyStopping(patience \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m15\u001b[39m, restore_best_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, verbose \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m          \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mval_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m          \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m          \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mearly_stopping_callbacks\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Uni/Prowiss/Code/Brain_Mets_Classification/brain_mets_env/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/Desktop/Uni/Prowiss/Code/Brain_Mets_Classification/brain_mets_env/lib/python3.11/site-packages/keras/src/engine/training.py:1807\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1799\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1800\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1801\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1804\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1805\u001b[0m ):\n\u001b[1;32m   1806\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1807\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1808\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1809\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/Desktop/Uni/Prowiss/Code/Brain_Mets_Classification/brain_mets_env/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/Desktop/Uni/Prowiss/Code/Brain_Mets_Classification/brain_mets_env/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:832\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    829\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    831\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 832\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    834\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    835\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/Desktop/Uni/Prowiss/Code/Brain_Mets_Classification/brain_mets_env/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:868\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    865\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    866\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    867\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 868\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_config\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    872\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    873\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    874\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/Desktop/Uni/Prowiss/Code/Brain_Mets_Classification/brain_mets_env/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Uni/Prowiss/Code/Brain_Mets_Classification/brain_mets_env/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1323\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1319\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1321\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1322\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1323\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1325\u001b[0m     args,\n\u001b[1;32m   1326\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1327\u001b[0m     executing_eagerly)\n\u001b[1;32m   1328\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/Desktop/Uni/Prowiss/Code/Brain_Mets_Classification/brain_mets_env/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m~/Desktop/Uni/Prowiss/Code/Brain_Mets_Classification/brain_mets_env/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/Uni/Prowiss/Code/Brain_Mets_Classification/brain_mets_env/lib/python3.11/site-packages/tensorflow/python/eager/context.py:1486\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1484\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1486\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1487\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1488\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1489\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1490\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1491\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1492\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1493\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1494\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1495\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1496\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1500\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1501\u001b[0m   )\n",
      "File \u001b[0;32m~/Desktop/Uni/Prowiss/Code/Brain_Mets_Classification/brain_mets_env/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-4\n",
    "\n",
    "optimizer = tf.optimizers.legacy.Adam(learning_rate = learning_rate)\n",
    "\n",
    "model_vit.compile(optimizer = optimizer, \n",
    "              loss = \"sparse_categorical_crossentropy\", \n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "\n",
    "early_stopping_callbacks = tf.keras.callbacks.EarlyStopping(patience = 15, restore_best_weights = True, verbose = 1)\n",
    "\n",
    "model_vit.fit(x = train_data,\n",
    "          validation_data = val_data,\n",
    "          epochs = EPOCHS,\n",
    "          callbacks = early_stopping_callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet50V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50v2_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "94668760/94668760 [==============================] - 6s 0us/step\n"
     ]
    }
   ],
   "source": [
    "resnet = tf.keras.applications.ResNet50V2(\n",
    "    include_top=False,\n",
    "    input_shape=(224,224,3),\n",
    "    pooling='avg',\n",
    "    weights='imagenet'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"resnet\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " sequential_28 (Sequential)  (None, 224, 224, 3)       0         \n",
      "                                                                 \n",
      " resnet50v2 (Functional)     (None, 2048)              23564800  \n",
      "                                                                 \n",
      " flatten_14 (Flatten)        (None, 2048)              0         \n",
      "                                                                 \n",
      " batch_normalization_28 (Ba  (None, 2048)              8192      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_56 (Dense)            (None, 128)               262272    \n",
      "                                                                 \n",
      " batch_normalization_29 (Ba  (None, 128)               512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense_57 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_58 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_59 (Dense)            (None, 3)                 99        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 23846211 (90.97 MB)\n",
      "Trainable params: 23796419 (90.78 MB)\n",
      "Non-trainable params: 49792 (194.50 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_resnet = tf.keras.Sequential([\n",
    "    tf.keras.layers.InputLayer(input_shape = (240, 240, 3)),\n",
    "    data_augmentation,\n",
    "    resnet,\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(128, activation = tf.keras.activations.gelu),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "    tf.keras.layers.Dense(64, activation = tf.keras.activations.gelu),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "    tf.keras.layers.Dense(32, activation = tf.keras.activations.gelu),\n",
    "    tf.keras.layers.Dense(3, activation = 'softmax')\n",
    "], name = \"resnet\")\n",
    "\n",
    "model_resnet.build()\n",
    "\n",
    "model_resnet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "58/58 [==============================] - 61s 997ms/step - loss: 1.4287 - accuracy: 0.3326 - val_loss: 1.1051 - val_accuracy: 0.3125\n",
      "Epoch 2/30\n",
      "58/58 [==============================] - 56s 965ms/step - loss: 1.3176 - accuracy: 0.3769 - val_loss: 1.0731 - val_accuracy: 0.4250\n",
      "Epoch 3/30\n",
      "58/58 [==============================] - 58s 994ms/step - loss: 1.2791 - accuracy: 0.3855 - val_loss: 1.0531 - val_accuracy: 0.6125\n",
      "Epoch 4/30\n",
      "58/58 [==============================] - 57s 980ms/step - loss: 1.3141 - accuracy: 0.3585 - val_loss: 0.9283 - val_accuracy: 0.6250\n",
      "Epoch 5/30\n",
      "58/58 [==============================] - 57s 974ms/step - loss: 1.2650 - accuracy: 0.3629 - val_loss: 1.0888 - val_accuracy: 0.3375\n",
      "Epoch 6/30\n",
      "58/58 [==============================] - 56s 970ms/step - loss: 1.2817 - accuracy: 0.3488 - val_loss: 1.0709 - val_accuracy: 0.4125\n",
      "Epoch 7/30\n",
      " 8/58 [===>..........................] - ETA: 49s - loss: 1.1981 - accuracy: 0.3516"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[361], line 12\u001b[0m\n\u001b[1;32m      5\u001b[0m model_resnet\u001b[38;5;241m.\u001b[39mcompile(optimizer \u001b[38;5;241m=\u001b[39m optimizer, \n\u001b[1;32m      6\u001b[0m               loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse_categorical_crossentropy\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m      7\u001b[0m               metrics \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     10\u001b[0m early_stopping_callbacks \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mEarlyStopping(patience \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m15\u001b[39m, restore_best_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, verbose \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m \u001b[43mmodel_resnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m          \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mval_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m          \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m          \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mearly_stopping_callbacks\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Uni/Prowiss/Code/Brain_Mets_Classification/brain_mets_env/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/Desktop/Uni/Prowiss/Code/Brain_Mets_Classification/brain_mets_env/lib/python3.11/site-packages/keras/src/engine/training.py:1807\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1799\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1800\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1801\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1804\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1805\u001b[0m ):\n\u001b[1;32m   1806\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1807\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1808\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1809\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/Desktop/Uni/Prowiss/Code/Brain_Mets_Classification/brain_mets_env/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/Desktop/Uni/Prowiss/Code/Brain_Mets_Classification/brain_mets_env/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:832\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    829\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    831\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 832\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    834\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    835\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/Desktop/Uni/Prowiss/Code/Brain_Mets_Classification/brain_mets_env/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:868\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    865\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    866\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    867\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 868\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_config\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    872\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    873\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    874\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/Desktop/Uni/Prowiss/Code/Brain_Mets_Classification/brain_mets_env/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Uni/Prowiss/Code/Brain_Mets_Classification/brain_mets_env/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1323\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1319\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1321\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1322\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1323\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1325\u001b[0m     args,\n\u001b[1;32m   1326\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1327\u001b[0m     executing_eagerly)\n\u001b[1;32m   1328\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/Desktop/Uni/Prowiss/Code/Brain_Mets_Classification/brain_mets_env/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m~/Desktop/Uni/Prowiss/Code/Brain_Mets_Classification/brain_mets_env/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/Uni/Prowiss/Code/Brain_Mets_Classification/brain_mets_env/lib/python3.11/site-packages/tensorflow/python/eager/context.py:1486\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1484\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1486\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1487\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1488\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1489\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1490\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1491\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1492\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1493\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1494\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1495\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1496\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1500\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1501\u001b[0m   )\n",
      "File \u001b[0;32m~/Desktop/Uni/Prowiss/Code/Brain_Mets_Classification/brain_mets_env/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-4\n",
    "\n",
    "optimizer = tf.optimizers.legacy.Adam(learning_rate = learning_rate)\n",
    "\n",
    "model_resnet.compile(optimizer = optimizer, \n",
    "              loss = \"sparse_categorical_crossentropy\", \n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "\n",
    "early_stopping_callbacks = tf.keras.callbacks.EarlyStopping(patience = 15, restore_best_weights = True, verbose = 1)\n",
    "\n",
    "model_resnet.fit(x = train_data,\n",
    "          validation_data = val_data,\n",
    "          epochs = EPOCHS,\n",
    "          callbacks = early_stopping_callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretrained models\n",
    "https://www.kaggle.com/models/tensorflow/inception \\\n",
    "https://www.kaggle.com/models/google/inception-v3 \\\n",
    "https://www.kaggle.com/models/google/bit \\\n",
    "https://www.kaggle.com/models/google/efficientnet-v2 \\\n",
    "https://www.kaggle.com/models/spsayakpaul/vision-transformer \\\n",
    "https://www.kaggle.com/models/spsayakpaul/convnext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "brain_mets_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
