{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2D CNN training\n",
    "This notebook works as a proof of concept version for the actual script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import random\n",
    "import os\n",
    "import glob\n",
    "from functools import partial\n",
    "from time import strftime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_tfrs = \"/Users/LennartPhilipp/Desktop/Uni/Prowiss/Datensatz_RGB/regensburg_slices_tfrecords/all_pats_single_gray\"\n",
    "path_to_logs = \"/Users/LennartPhilipp/Desktop/Uni/Prowiss/Datensatz_RGB/regensburg_slices_tfrecords/test_logs\"\n",
    "\n",
    "training_codename = \"2D_pretrained_0000\"\n",
    "\n",
    "num_classes = 3\n",
    "sequence_to_train_on = \"t1c\"\n",
    "\n",
    "time = strftime(\"run_%Y_%m_%d_%H_%M_%S\")\n",
    "class_directory = f\"{training_codename}_{num_classes}_classes_{time}\"\n",
    "\n",
    "path_to_callbacks = Path(path_to_logs) / Path(class_directory)\n",
    "os.makedirs(path_to_callbacks, exist_ok=True)\n",
    "\n",
    "input_shape = (240, 240, 4)\n",
    "\n",
    "\n",
    "\n",
    "## train / val / test split\n",
    "train_ratio = 0.8\n",
    "val_ratio = 0.1\n",
    "test_ratio = 0.1\n",
    "\n",
    "activation_func = \"mish\"\n",
    "learning_rate = 0.001\n",
    "\n",
    "early_stopping_patience = 150\n",
    "shuffle_buffer_size = 20\n",
    "repeat_count = 1\n",
    "\n",
    "image_size = 224\n",
    "batch_size = 16\n",
    "EPOCHS = 30\n",
    "\n",
    "len_train = 0\n",
    "len_val = 0\n",
    "len_test = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_patient_paths():\n",
    "    patients = [f for f in os.listdir(path_to_tfrs) if os.path.isdir(os.path.join(path_to_tfrs, f))]\n",
    "\n",
    "    patient_paths = [str(path_to_tfrs) + \"/\" + patient for patient in patients]\n",
    "\n",
    "    print(f\"total patients: {len(patient_paths)}\")\n",
    "\n",
    "    for path in patient_paths:\n",
    "        patient_not_empty = False\n",
    "        patient_files = os.listdir(path)\n",
    "        for file in patient_files:\n",
    "            if file.endswith(\".tfrecord\"):\n",
    "                patient_not_empty = True\n",
    "        \n",
    "        if patient_not_empty == False:\n",
    "            patient_paths.remove(path)\n",
    "\n",
    "    return patient_paths\n",
    "\n",
    "def split_patients(patient_paths, fraction_to_use = 1):\n",
    "\n",
    "    random.shuffle(patient_paths)\n",
    "\n",
    "    patient_paths = patient_paths[:int(len(patient_paths) * fraction_to_use)]\n",
    "\n",
    "    if fraction_to_use != 1:\n",
    "        print(f\"actual tfrs length: {len(patient_paths)}\")\n",
    "\n",
    "    train_size = int(len(patient_paths) * train_ratio)\n",
    "    val_size = int(len(patient_paths) * val_ratio)\n",
    "\n",
    "    train_patients_paths = patient_paths[:train_size]\n",
    "    val_patients_paths = patient_paths[train_size:train_size + val_size]\n",
    "    test_patients_paths = patient_paths[train_size + val_size:]\n",
    "\n",
    "    print(f\"train: {len(train_patients_paths)} | val: {len(val_patients_paths)} | test: {len(test_patients_paths)}\")\n",
    "\n",
    "    # save train / val / test patients to txt file\n",
    "    # hf.save_paths_to_txt(train_patients_paths, \"train\", path_to_callbacks)\n",
    "    # hf.save_paths_to_txt(val_patients_paths, \"val\", path_to_callbacks)\n",
    "    # hf.save_paths_to_txt(test_patients_paths, \"test\", path_to_callbacks)\n",
    "\n",
    "    sum = len(train_patients_paths) + len(val_patients_paths) + len(test_patients_paths)\n",
    "    if sum != len(patient_paths):\n",
    "        print(\"WARNING: error occured in train / val / test split!\")\n",
    "\n",
    "    return train_patients_paths, val_patients_paths, test_patients_paths\n",
    "\n",
    "def get_tfr_paths_for_patients(patient_paths):\n",
    "\n",
    "    tfr_paths = []\n",
    "\n",
    "    for patient in patient_paths:\n",
    "        tfr_paths.extend(glob.glob(patient + \"/*.tfrecord\"))\n",
    "    \n",
    "    for path in tfr_paths:\n",
    "        verify_tfrecord(path)\n",
    "\n",
    "    #print(f\"total tfrs: {len(tfr_paths)}\")\n",
    "\n",
    "    return tfr_paths\n",
    "\n",
    "def read_data(train_paths, val_paths, test_paths = None):\n",
    "\n",
    "    train_data = tf.data.Dataset.from_tensor_slices(train_paths)\n",
    "    val_data = tf.data.Dataset.from_tensor_slices(val_paths)\n",
    "\n",
    "    train_data = train_data.interleave(\n",
    "        lambda x: tf.data.TFRecordDataset([x], compression_type=\"GZIP\"),\n",
    "        num_parallel_calls=tf.data.AUTOTUNE,\n",
    "        deterministic=False\n",
    "    )\n",
    "    val_data = val_data.interleave(\n",
    "        lambda x: tf.data.TFRecordDataset([x], compression_type=\"GZIP\"),\n",
    "        num_parallel_calls=tf.data.AUTOTUNE,\n",
    "        deterministic=False\n",
    "    )\n",
    "\n",
    "    train_data = train_data.map(partial(parse_record, image_only = False, labeled = True, num_classes = num_classes), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    val_data = val_data.map(partial(parse_record, image_only = False, labeled = True, num_classes = num_classes), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "    train_data = train_data.shuffle(buffer_size=shuffle_buffer_size)\n",
    "    val_data = val_data.shuffle(buffer_size=shuffle_buffer_size)\n",
    "\n",
    "    train_data = train_data.repeat(count = repeat_count)\n",
    "    val_data = val_data.repeat(count = repeat_count)\n",
    "\n",
    "    train_data = train_data.batch(batch_size)\n",
    "    val_data = val_data.batch(batch_size)\n",
    "\n",
    "    train_data = train_data.prefetch(buffer_size=1)\n",
    "    val_data = val_data.prefetch(buffer_size=1)\n",
    "\n",
    "    if test_paths is not None:\n",
    "        test_data = tf.data.Dataset.from_tensor_slices(test_paths)\n",
    "        test_data = test_data.interleave(\n",
    "            lambda x: tf.data.TFRecordDataset([x], compression_type=\"GZIP\"),\n",
    "            num_parallel_calls=tf.data.AUTOTUNE,\n",
    "            deterministic=False\n",
    "        )\n",
    "        test_data = test_data.map(partial(parse_record, image_only = False, labeled = True, num_classes = num_classes), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        test_data = test_data.batch(batch_size)\n",
    "        test_data = test_data.prefetch(buffer_size=1)\n",
    "\n",
    "        return train_data, val_data, test_data\n",
    "\n",
    "    return train_data, val_data\n",
    "\n",
    "def parse_record(record, image_only = False, labeled = False, num_classes = 2, rgb = False, sequence = \"t1c\"):\n",
    "\n",
    "    image_shape = []\n",
    "\n",
    "    if rgb: # rgb images need three channels\n",
    "        image_shape = [240, 240, 3, 4]\n",
    "    else: # gray scale images don't\n",
    "        image_shape = [240, 240, 4]\n",
    "\n",
    "    feature_description = {\n",
    "        \"image\": tf.io.FixedLenFeature(image_shape, tf.float32),\n",
    "        \"sex\": tf.io.FixedLenFeature([], tf.int64, default_value=[0]),\n",
    "        \"age\": tf.io.FixedLenFeature([], tf.int64, default_value=0),\n",
    "        \"primary\": tf.io.FixedLenFeature([], tf.int64, default_value=0),\n",
    "    }\n",
    "\n",
    "    example = tf.io.parse_single_example(record, feature_description)\n",
    "    image = example[\"image\"]\n",
    "    image = tf.reshape(image, image_shape)\n",
    "\n",
    "    # primary should have a value between 0 and 5\n",
    "    # depending on num classes return different values\n",
    "    # if num_classes = 2, return 1 if primary is 1, else 0\n",
    "    # if num_classes = 3, return primaries 1 and 2, else 0\n",
    "    # if num_classes = 4, return primaries 1, 2 and 3, else 0\n",
    "    # if num_classes = 5, return primaries 1, 2, 3 and 4, else 0\n",
    "    # if num_classes = 6, return primaries 1, 2, 3, 4 and 5, else 0\n",
    "\n",
    "    primary_to_return = tf.constant(0, dtype=tf.int64)\n",
    "\n",
    "    if num_classes == 2:\n",
    "        if example[\"primary\"] == tf.constant(1, dtype=tf.int64):\n",
    "            primary_to_return = example[\"primary\"]\n",
    "        else:\n",
    "            primary_to_return = tf.constant(0, dtype=tf.int64)\n",
    "    elif num_classes == 3:\n",
    "        if example[\"primary\"] == tf.constant(1, dtype=tf.int64) or example[\"primary\"] == tf.constant(2, dtype=tf.int64):\n",
    "            primary_to_return = example[\"primary\"]\n",
    "        else:\n",
    "            primary_to_return = tf.constant(0, dtype=tf.int64)\n",
    "    elif num_classes == 4:\n",
    "        if example[\"primary\"] == tf.constant(1, dtype=tf.int64) or example[\"primary\"] == tf.constant(2, dtype=tf.int64) or example[\"primary\"] == tf.constant(3, dtype=tf.int64):\n",
    "            primary_to_return = example[\"primary\"]\n",
    "        else:\n",
    "            primary_to_return = tf.constant(0, dtype=tf.int64)\n",
    "    elif num_classes == 5:\n",
    "        if example[\"primary\"] == tf.constant(1, dtype=tf.int64) or example[\"primary\"] == tf.constant(2, dtype=tf.int64) or example[\"primary\"] == tf.constant(3, dtype=tf.int64) or example[\"primary\"] == tf.constant(4, dtype=tf.int64):\n",
    "            primary_to_return = example[\"primary\"]\n",
    "        else:\n",
    "            primary_to_return = tf.constant(0, dtype=tf.int64)\n",
    "    elif num_classes == 6:\n",
    "        if example[\"primary\"] == tf.constant(1, dtype=tf.int64) or example[\"primary\"] == tf.constant(2, dtype=tf.int64) or example[\"primary\"] == tf.constant(3, dtype=tf.int64) or example[\"primary\"] == tf.constant(4, dtype=tf.int64) or example[\"primary\"] == tf.constant(5, dtype=tf.int64):\n",
    "            primary_to_return = example[\"primary\"]\n",
    "        else:\n",
    "            primary_to_return = tf.constant(0, dtype=tf.int64)\n",
    "    else:\n",
    "            print(\"ERROR\")\n",
    "            print(\"num classes not supported\")\n",
    "            print(\"Check parse_record function\")\n",
    "            print(\"____________________________\")\n",
    "\n",
    "    if rgb: # select the right sequence to return\n",
    "        if sequence == \"t1\":\n",
    "            image = image[:, :, :, 0]\n",
    "        elif sequence == \"t1c\":\n",
    "            image = image[:, :, :, 1]\n",
    "        elif sequence == \"t2\":\n",
    "            image = image[:, :, :, 2]\n",
    "        elif sequence == \"flair\":\n",
    "            image = image[:, :, :, 3]\n",
    "\n",
    "    if image_only:\n",
    "        return image, primary_to_return\n",
    "    elif labeled:\n",
    "        return (image, example[\"sex\"], example[\"age\"]), primary_to_return #example[\"primary\"]\n",
    "    else:\n",
    "        return image\n",
    "    \n",
    "def verify_tfrecord(file_path):\n",
    "    try:\n",
    "        for _ in tf.data.TFRecordDataset(file_path, compression_type=\"GZIP\"):\n",
    "            pass\n",
    "    except tf.errors.DataLossError:\n",
    "        print(f\"Corrupted TFRecord file: {file_path}\")\n",
    "\n",
    "def get_callbacks(fold_num = 0,\n",
    "                  use_checkpoint = True,\n",
    "                  use_early_stopping = True,\n",
    "                  early_stopping_patience = early_stopping_patience,\n",
    "                  use_tensorboard = True,\n",
    "                  use_csv_logger = True,\n",
    "                  use_lrscheduler = False):\n",
    "\n",
    "    callbacks = []\n",
    "\n",
    "    path_to_fold_callbacks = path_to_callbacks / f\"fold_{fold_num}\"\n",
    "\n",
    "    def get_run_logdir(root_logdir = path_to_fold_callbacks / \"tensorboard\"):\n",
    "        return Path(root_logdir) / strftime(\"run_%Y_%m_%d_%H_%M_%S\")\n",
    "\n",
    "    run_logdir = get_run_logdir()\n",
    "\n",
    "    # model checkpoint\n",
    "    if use_checkpoint:\n",
    "        checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n",
    "            filepath = path_to_fold_callbacks / \"saved_weights.weights.h5\",\n",
    "            monitor = \"val_accuracy\",\n",
    "            mode = \"max\",\n",
    "            save_best_only = True,\n",
    "            save_weights_only = True,\n",
    "        )\n",
    "        callbacks.append(checkpoint_cb)\n",
    "\n",
    "    # early stopping\n",
    "    if use_early_stopping:\n",
    "        early_stopping_cb = tf.keras.callbacks.EarlyStopping(\n",
    "            patience = early_stopping_patience,\n",
    "            restore_best_weights = True,\n",
    "            verbose = 1\n",
    "        )\n",
    "        callbacks.append(early_stopping_cb)\n",
    "\n",
    "    # tensorboard, doesn't really work yet\n",
    "    if use_tensorboard:\n",
    "        tensorboard_cb = tf.keras.callbacks.TensorBoard(log_dir = run_logdir,\n",
    "                                                    histogram_freq = 1)\n",
    "        callbacks.append(tensorboard_cb)\n",
    "    \n",
    "    # csv logger\n",
    "    if use_csv_logger:\n",
    "        csv_logger_cb = tf.keras.callbacks.CSVLogger(path_to_fold_callbacks / \"training.csv\", separator = \",\", append = True)\n",
    "        callbacks.append(csv_logger_cb)\n",
    "    \n",
    "    if use_lrscheduler:\n",
    "        lr_schedule = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-8 * 10**(epoch * 0.0175))\n",
    "        callbacks.append(lr_schedule)\n",
    "\n",
    "    print(\"get_callbacks successful\")\n",
    "\n",
    "    return callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalizeToRange(tf.keras.layers.Layer):\n",
    "    def __init__(self, zero_to_one=True):\n",
    "        super(NormalizeToRange, self).__init__()\n",
    "        self.zero_to_one = zero_to_one\n",
    "\n",
    "    def call(self, inputs):\n",
    "        min_val = tf.reduce_min(inputs)\n",
    "        max_val = tf.reduce_max(inputs)\n",
    "        if self.zero_to_one:\n",
    "            # Normalize to [0, 1]\n",
    "            normalized = (inputs - min_val) / (max_val - min_val)\n",
    "        else:\n",
    "            # Normalize to [-1, 1]\n",
    "            normalized = 2 * (inputs - min_val) / (max_val - min_val) - 1\n",
    "        return normalized\n",
    "    \n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    tf.keras.layers.RandomFlip(mode = \"horizontal\"),\n",
    "    #tf.keras.layers.Rescaling(1/255),\n",
    "    tf.keras.layers.RandomContrast(0.5), # consider removing the random contrast layer as that causes pixel values to go beyond 1\n",
    "    tf.keras.layers.RandomBrightness(factor = (-0.2, 0.4)), #, value_range=(0, 1)\n",
    "    tf.keras.layers.RandomRotation(factor = (-0.1, 0.1), fill_mode = \"nearest\"),\n",
    "    NormalizeToRange(zero_to_one=True),\n",
    "    tf.keras.layers.RandomTranslation(\n",
    "        height_factor = 0.05,\n",
    "        width_factor = 0.05,\n",
    "        fill_mode = \"nearest\",\n",
    "        interpolation = \"bilinear\"\n",
    "    ),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total patients: 471\n",
      "train: 370 | val: 46 | test: 47\n",
      "train: 1537 | val: 132 | test: 158\n",
      "get_callbacks successful\n"
     ]
    }
   ],
   "source": [
    "patients = get_patient_paths()\n",
    "\n",
    "train_patients, val_patients, test_patients = split_patients(patients, fraction_to_use = 1)\n",
    "\n",
    "train_paths = get_tfr_paths_for_patients(train_patients)\n",
    "val_paths = get_tfr_paths_for_patients(val_patients)\n",
    "test_paths = get_tfr_paths_for_patients(test_patients)\n",
    "train_data, val_data, test_data = read_data(train_paths, val_paths, test_paths)\n",
    "\n",
    "len_train = len(train_paths)\n",
    "len_val = len(val_paths)\n",
    "len_test = len(test_paths)\n",
    "print(f\"train: {len_train} | val: {len_val} | test: {len_test}\")\n",
    "\n",
    "callbacks = get_callbacks(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preview single image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "(16, 240, 240, 4)\n",
      "(240, 240, 4)\n",
      "\n",
      "Before Augmentation\n",
      "Min: 0.0\n",
      "Max: 255.0\n",
      "Mean: 3.0679688453674316\n",
      "________________________________________\n",
      "\n",
      "After Augmentation\n",
      "Min: 0.0\n",
      "Max: 1.0\n",
      "Mean: 0.015604679472744465\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_image = train_data.take(1)\n",
    "\n",
    "for (image, sex, age), primary in test_image:\n",
    "    print(primary.numpy()[0])\n",
    "    print(image.shape)\n",
    "    print(image[0].shape)\n",
    "    image = image[0]\n",
    "    imagebefore = image\n",
    "\n",
    "    print()\n",
    "    print(\"Before Augmentation\")\n",
    "\n",
    "    print(f\"Min: {tf.reduce_min(image)}\")\n",
    "    print(f\"Max: {tf.reduce_max(image)}\")\n",
    "    print(f\"Mean: {tf.reduce_mean(image)}\")\n",
    "\n",
    "    print(\"________________________________________\")\n",
    "    print()\n",
    "    print(\"After Augmentation\")\n",
    "    image = data_augmentation(image)\n",
    "    print(f\"Min: {tf.reduce_min(image)}\")\n",
    "    print(f\"Max: {tf.reduce_max(image)}\")\n",
    "    print(f\"Mean: {tf.reduce_mean(image)}\")\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    fig.suptitle('Before / After Augmentation')\n",
    "    ax1.imshow(imagebefore[:,:,1], cmap = \"gray\")\n",
    "    ax2.imshow(image[:,:,1], cmap = \"gray\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### normal Conv model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_conv_model():\n",
    "\n",
    "    DefaultConv2D = partial(tf.keras.layers.Conv2D, kernel_size=3, padding=\"same\", activation = activation_func, kernel_initializer=\"he_normal\")\n",
    "\n",
    "    optimizer = tf.keras.optimizers.legacy.SGD(learning_rate=learning_rate, momentum=0.9, nesterov=True)\n",
    "\n",
    "    # Define inputs\n",
    "    image_input = tf.keras.layers.Input(shape=(240, 240, 4))\n",
    "    sex_input = tf.keras.layers.Input(shape=(1,))\n",
    "    age_input = tf.keras.layers.Input(shape=(1,))\n",
    "\n",
    "    batch_norm_layer = tf.keras.layers.BatchNormalization()\n",
    "    conv_1_layer = DefaultConv2D(filters = 64, kernel_size = 7, strides = 2, input_shape = [240, 240, 4])\n",
    "    max_pool_1_layer = tf.keras.layers.MaxPool2D(pool_size = (2,2))\n",
    "\n",
    "    conv_2_layer = DefaultConv2D(filters = 128)\n",
    "    conv_3_layer = DefaultConv2D(filters = 128)\n",
    "    max_pool_2_layer = tf.keras.layers.MaxPool2D(pool_size = (2,2))\n",
    "\n",
    "    conv_4_layer = DefaultConv2D(filters = 256)\n",
    "    conv_5_layer = DefaultConv2D(filters = 256)\n",
    "    max_pool_3_layer = tf.keras.layers.MaxPool2D(pool_size = (2,2))\n",
    "    \n",
    "    # conv_4_layer = tf.keras.layers.Conv2D(filters = 256, kernel_size = 3, strides=(1,1,1), activation=activation_func, kernel_initializer=tf.keras.initializers.HeNormal())\n",
    "    # max_pool_4_layer = tf.keras.layers.MaxPool2D(pool_size = (2,2,2))\n",
    "\n",
    "    dense_1_layer = tf.keras.layers.Dense(256, activation=activation_func, kernel_initializer=tf.keras.initializers.HeNormal())\n",
    "    dropout_1_layer = tf.keras.layers.Dropout(0.5)\n",
    "    dense_2_layer = tf.keras.layers.Dense(128, activation=activation_func, kernel_initializer=tf.keras.initializers.HeNormal())\n",
    "    dropout_2_layer = tf.keras.layers.Dropout(0.5)\n",
    "\n",
    "\n",
    "    batch_norm = batch_norm_layer(image_input)\n",
    "\n",
    "    conv_1 = conv_1_layer(batch_norm)\n",
    "    max_pool_1 = max_pool_1_layer(conv_1)\n",
    "\n",
    "    conv_2 = conv_2_layer(max_pool_1)\n",
    "    conv_3 = conv_3_layer(conv_2)\n",
    "    max_pool_2 = max_pool_2_layer(conv_3)\n",
    "\n",
    "    conv_4 = conv_4_layer(max_pool_2)\n",
    "    conv_5 = conv_5_layer(conv_4)\n",
    "    max_pool_3 = max_pool_3_layer(conv_5)\n",
    "\n",
    "    flatten = tf.keras.layers.Flatten()(max_pool_3)\n",
    "\n",
    "    flattened_sex_input = tf.keras.layers.Flatten()(sex_input)\n",
    "    age_input_reshaped = tf.keras.layers.Reshape((1,))(age_input)  # Reshape age_input to have 2 dimensions\n",
    "    concatenated_inputs = tf.keras.layers.Concatenate()([flatten, age_input_reshaped, flattened_sex_input])\n",
    "\n",
    "    x = dense_1_layer(concatenated_inputs)\n",
    "    x = dropout_1_layer(x)\n",
    "    x = dense_2_layer(x)\n",
    "    x = dropout_2_layer(x)\n",
    "\n",
    "    match num_classes:\n",
    "        case 2:\n",
    "            x = tf.keras.layers.Dense(1)(x)\n",
    "            output = tf.keras.layers.Activation('sigmoid', dtype='float32', name='predictions')(x)\n",
    "        case 3:\n",
    "            x = tf.keras.layers.Dense(3)(x)\n",
    "            output = tf.keras.layers.Activation('softmax', dtype='float32', name='predictions')(x)\n",
    "        case 4:\n",
    "            x = tf.keras.layers.Dense(4)(x)\n",
    "            output = tf.keras.layers.Activation('softmax', dtype='float32', name='predictions')(x)\n",
    "        case 5:\n",
    "            x = tf.keras.layers.Dense(5)(x)\n",
    "            output = tf.keras.layers.Activation('softmax', dtype='float32', name='predictions')(x)\n",
    "        case 6:\n",
    "            x = tf.keras.layers.Dense(6)(x)\n",
    "            output = tf.keras.layers.Activation('softmax', dtype='float32', name='predictions')(x)\n",
    "        case _:\n",
    "            print(\"Wrong num classes set in the buil_ai func, please pick a number between 2 and 6\")\n",
    "\n",
    "    model = tf.keras.Model(inputs = [image_input, sex_input, age_input], outputs = [output])\n",
    "\n",
    "    if num_classes > 2:\n",
    "        model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics = [\"RootMeanSquaredError\", \"accuracy\"])\n",
    "    else:\n",
    "        model.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics = [\"RootMeanSquaredError\", \"accuracy\"])\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNeXt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 240, 240, 4)]        0         []                            \n",
      "                                                                                                  \n",
      " batch_normalization (Batch  (None, 240, 240, 4)          16        ['input_1[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)             (None, 120, 120, 64)         12608     ['batch_normalization[0][0]'] \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2  (None, 60, 60, 64)           0         ['conv2d[0][0]']              \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)           (None, 60, 60, 128)          73856     ['max_pooling2d[0][0]']       \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)           (None, 60, 60, 128)          147584    ['conv2d_1[0][0]']            \n",
      "                                                                                                  \n",
      " max_pooling2d_1 (MaxPoolin  (None, 30, 30, 128)          0         ['conv2d_2[0][0]']            \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)           (None, 30, 30, 256)          295168    ['max_pooling2d_1[0][0]']     \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)           (None, 30, 30, 256)          590080    ['conv2d_3[0][0]']            \n",
      "                                                                                                  \n",
      " max_pooling2d_2 (MaxPoolin  (None, 15, 15, 256)          0         ['conv2d_4[0][0]']            \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " input_3 (InputLayer)        [(None, 1)]                  0         []                            \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)        [(None, 1)]                  0         []                            \n",
      "                                                                                                  \n",
      " flatten (Flatten)           (None, 57600)                0         ['max_pooling2d_2[0][0]']     \n",
      "                                                                                                  \n",
      " reshape (Reshape)           (None, 1)                    0         ['input_3[0][0]']             \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)         (None, 1)                    0         ['input_2[0][0]']             \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)   (None, 57602)                0         ['flatten[0][0]',             \n",
      "                                                                     'reshape[0][0]',             \n",
      "                                                                     'flatten_1[0][0]']           \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 256)                  1474636   ['concatenate[0][0]']         \n",
      "                                                          8                                       \n",
      "                                                                                                  \n",
      " dropout (Dropout)           (None, 256)                  0         ['dense[0][0]']               \n",
      "                                                                                                  \n",
      " dense_1 (Dense)             (None, 128)                  32896     ['dropout[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)         (None, 128)                  0         ['dense_1[0][0]']             \n",
      "                                                                                                  \n",
      " dense_2 (Dense)             (None, 3)                    387       ['dropout_1[0][0]']           \n",
      "                                                                                                  \n",
      " predictions (Activation)    (None, 3)                    0         ['dense_2[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 15898963 (60.65 MB)\n",
      "Trainable params: 15898955 (60.65 MB)\n",
      "Non-trainable params: 8 (32.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/30\n",
      "97/97 [==============================] - 48s 491ms/step - loss: 2.1317 - root_mean_squared_error: 0.9062 - accuracy: 0.4489 - val_loss: 1.2017 - val_root_mean_squared_error: 0.9290 - val_accuracy: 0.2121\n",
      "Epoch 2/30\n",
      "97/97 [==============================] - 45s 468ms/step - loss: 1.0623 - root_mean_squared_error: 0.8828 - accuracy: 0.4997 - val_loss: 1.0695 - val_root_mean_squared_error: 0.9177 - val_accuracy: 0.2500\n",
      "Epoch 3/30\n",
      "97/97 [==============================] - 45s 462ms/step - loss: 1.0311 - root_mean_squared_error: 0.8786 - accuracy: 0.5270 - val_loss: 1.0112 - val_root_mean_squared_error: 0.9179 - val_accuracy: 0.5909\n",
      "Epoch 4/30\n",
      "97/97 [==============================] - 45s 465ms/step - loss: 1.0284 - root_mean_squared_error: 0.8737 - accuracy: 0.5374 - val_loss: 1.0245 - val_root_mean_squared_error: 0.9171 - val_accuracy: 0.5833\n",
      "Epoch 5/30\n",
      "97/97 [==============================] - 46s 469ms/step - loss: 1.0203 - root_mean_squared_error: 0.8737 - accuracy: 0.5329 - val_loss: 1.0016 - val_root_mean_squared_error: 0.9209 - val_accuracy: 0.5909\n",
      "Epoch 6/30\n",
      "97/97 [==============================] - 46s 471ms/step - loss: 1.0143 - root_mean_squared_error: 0.8728 - accuracy: 0.5413 - val_loss: 1.0304 - val_root_mean_squared_error: 0.9165 - val_accuracy: 0.5909\n",
      "Epoch 7/30\n",
      "97/97 [==============================] - 47s 485ms/step - loss: 1.0114 - root_mean_squared_error: 0.8714 - accuracy: 0.5407 - val_loss: 1.0109 - val_root_mean_squared_error: 0.9199 - val_accuracy: 0.5909\n",
      "Epoch 8/30\n",
      "18/97 [====>.........................] - ETA: 36s - loss: 0.9219 - root_mean_squared_error: 0.8350 - accuracy: 0.7292"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m build_conv_model()\n\u001b[0;32m----> 3\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m          \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mval_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m          \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m          \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Uni/Prowiss/Code/Brain_Mets_Classification/brain_mets_env/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/Desktop/Uni/Prowiss/Code/Brain_Mets_Classification/brain_mets_env/lib/python3.11/site-packages/keras/src/engine/training.py:1807\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1799\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1800\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1801\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1804\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1805\u001b[0m ):\n\u001b[1;32m   1806\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1807\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1808\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1809\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/Desktop/Uni/Prowiss/Code/Brain_Mets_Classification/brain_mets_env/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/Desktop/Uni/Prowiss/Code/Brain_Mets_Classification/brain_mets_env/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:832\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    829\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    831\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 832\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    834\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    835\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/Desktop/Uni/Prowiss/Code/Brain_Mets_Classification/brain_mets_env/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:868\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    865\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    866\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    867\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 868\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_config\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    872\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    873\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    874\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/Desktop/Uni/Prowiss/Code/Brain_Mets_Classification/brain_mets_env/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Uni/Prowiss/Code/Brain_Mets_Classification/brain_mets_env/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1323\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1319\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1321\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1322\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1323\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1325\u001b[0m     args,\n\u001b[1;32m   1326\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1327\u001b[0m     executing_eagerly)\n\u001b[1;32m   1328\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/Desktop/Uni/Prowiss/Code/Brain_Mets_Classification/brain_mets_env/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m~/Desktop/Uni/Prowiss/Code/Brain_Mets_Classification/brain_mets_env/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/Uni/Prowiss/Code/Brain_Mets_Classification/brain_mets_env/lib/python3.11/site-packages/tensorflow/python/eager/context.py:1486\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1484\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1486\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1487\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1488\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1489\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1490\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1491\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1492\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1493\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1494\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1495\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1496\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1500\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1501\u001b[0m   )\n",
      "File \u001b[0;32m~/Desktop/Uni/Prowiss/Code/Brain_Mets_Classification/brain_mets_env/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = build_conv_model()\n",
    "\n",
    "model.fit(x = train_data,\n",
    "          validation_data = val_data,\n",
    "          epochs = EPOCHS,\n",
    "          callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pretrained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vision Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the standard libraries, watchout though: vit_keras uses tensorflow_addons, which has stopped any development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import random\n",
    "import os\n",
    "import glob\n",
    "from functools import partial\n",
    "from time import strftime\n",
    "\n",
    "from vit_keras import vit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "helper functions below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_tfrs = \"/Users/LennartPhilipp/Desktop/Uni/Prowiss/Datensatz_RGB/regensburg_slices_tfrecords/all_pats_single_rgb\"\n",
    "path_to_logs = \"/Users/LennartPhilipp/Desktop/Uni/Prowiss/Datensatz_RGB/regensburg_slices_tfrecords/test_logs\"\n",
    "\n",
    "training_codename = \"2D_pretrained_0000\"\n",
    "\n",
    "num_classes = 3\n",
    "sequence_to_train_on = \"t1c\"\n",
    "\n",
    "time = strftime(\"run_%Y_%m_%d_%H_%M_%S\")\n",
    "class_directory = f\"{training_codename}_{num_classes}_classes_{time}\"\n",
    "\n",
    "path_to_callbacks = Path(path_to_logs) / Path(class_directory)\n",
    "os.makedirs(path_to_callbacks, exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "image_size = 224\n",
    "\n",
    "## train / val / test split\n",
    "train_ratio = 0.8\n",
    "val_ratio = 0.1\n",
    "test_ratio = 0.1\n",
    "\n",
    "early_stopping_patience = 150\n",
    "shuffle_buffer_size = 20\n",
    "repeat_count = 1\n",
    "\n",
    "image_size = 224\n",
    "batch_size = 16\n",
    "EPOCHS = 30\n",
    "\n",
    "len_train = 0\n",
    "len_val = 0\n",
    "len_test = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_patient_paths():\n",
    "    patients = [f for f in os.listdir(path_to_tfrs) if os.path.isdir(os.path.join(path_to_tfrs, f))]\n",
    "\n",
    "    patient_paths = [str(path_to_tfrs) + \"/\" + patient for patient in patients]\n",
    "\n",
    "    print(f\"total patients: {len(patient_paths)}\")\n",
    "\n",
    "    for path in patient_paths:\n",
    "        patient_not_empty = False\n",
    "        patient_files = os.listdir(path)\n",
    "        for file in patient_files:\n",
    "            if file.endswith(\".tfrecord\"):\n",
    "                patient_not_empty = True\n",
    "        \n",
    "        if patient_not_empty == False:\n",
    "            patient_paths.remove(path)\n",
    "\n",
    "    return patient_paths\n",
    "\n",
    "def split_patients(patient_paths, fraction_to_use = 1):\n",
    "\n",
    "    random.shuffle(patient_paths)\n",
    "\n",
    "    patient_paths = patient_paths[:int(len(patient_paths) * fraction_to_use)]\n",
    "\n",
    "    if fraction_to_use != 1:\n",
    "        print(f\"actual tfrs length: {len(patient_paths)}\")\n",
    "\n",
    "    train_size = int(len(patient_paths) * train_ratio)\n",
    "    val_size = int(len(patient_paths) * val_ratio)\n",
    "\n",
    "    train_patients_paths = patient_paths[:train_size]\n",
    "    val_patients_paths = patient_paths[train_size:train_size + val_size]\n",
    "    test_patients_paths = patient_paths[train_size + val_size:]\n",
    "\n",
    "    print(f\"train: {len(train_patients_paths)} | val: {len(val_patients_paths)} | test: {len(test_patients_paths)}\")\n",
    "\n",
    "    # save train / val / test patients to txt file\n",
    "    # hf.save_paths_to_txt(train_patients_paths, \"train\", path_to_callbacks)\n",
    "    # hf.save_paths_to_txt(val_patients_paths, \"val\", path_to_callbacks)\n",
    "    # hf.save_paths_to_txt(test_patients_paths, \"test\", path_to_callbacks)\n",
    "\n",
    "    sum = len(train_patients_paths) + len(val_patients_paths) + len(test_patients_paths)\n",
    "    if sum != len(patient_paths):\n",
    "        print(\"WARNING: error occured in train / val / test split!\")\n",
    "\n",
    "    return train_patients_paths, val_patients_paths, test_patients_paths\n",
    "\n",
    "def get_tfr_paths_for_patients(patient_paths):\n",
    "\n",
    "    tfr_paths = []\n",
    "\n",
    "    for patient in patient_paths:\n",
    "        tfr_paths.extend(glob.glob(patient + \"/*.tfrecord\"))\n",
    "    \n",
    "    for path in tfr_paths:\n",
    "        verify_tfrecord(path)\n",
    "\n",
    "    #print(f\"total tfrs: {len(tfr_paths)}\")\n",
    "\n",
    "    return tfr_paths\n",
    "\n",
    "def read_data(train_paths, val_paths, test_paths = None):\n",
    "\n",
    "    train_data = tf.data.Dataset.from_tensor_slices(train_paths)\n",
    "    val_data = tf.data.Dataset.from_tensor_slices(val_paths)\n",
    "\n",
    "    train_data = train_data.interleave(\n",
    "        lambda x: tf.data.TFRecordDataset([x], compression_type=\"GZIP\"),\n",
    "        num_parallel_calls=tf.data.AUTOTUNE,\n",
    "        deterministic=False\n",
    "    )\n",
    "    val_data = val_data.interleave(\n",
    "        lambda x: tf.data.TFRecordDataset([x], compression_type=\"GZIP\"),\n",
    "        num_parallel_calls=tf.data.AUTOTUNE,\n",
    "        deterministic=False\n",
    "    )\n",
    "\n",
    "    train_data = train_data.map(partial(parse_record, image_only = True, labeled = True, num_classes = num_classes), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    val_data = val_data.map(partial(parse_record, image_only = True, labeled = True, num_classes = num_classes), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "    train_data = train_data.shuffle(buffer_size=shuffle_buffer_size)\n",
    "    val_data = val_data.shuffle(buffer_size=shuffle_buffer_size)\n",
    "\n",
    "    train_data = train_data.repeat(count = repeat_count)\n",
    "    val_data = val_data.repeat(count = repeat_count)\n",
    "\n",
    "    train_data = train_data.batch(batch_size)\n",
    "    val_data = val_data.batch(batch_size)\n",
    "\n",
    "    train_data = train_data.prefetch(buffer_size=1)\n",
    "    val_data = val_data.prefetch(buffer_size=1)\n",
    "\n",
    "    if test_paths is not None:\n",
    "        test_data = tf.data.Dataset.from_tensor_slices(test_paths)\n",
    "        test_data = test_data.interleave(\n",
    "            lambda x: tf.data.TFRecordDataset([x], compression_type=\"GZIP\"),\n",
    "            num_parallel_calls=tf.data.AUTOTUNE,\n",
    "            deterministic=False\n",
    "        )\n",
    "        test_data = test_data.map(partial(parse_record, image_only = True, labeled = True, num_classes = num_classes), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        test_data = test_data.batch(batch_size)\n",
    "        test_data = test_data.prefetch(buffer_size=1)\n",
    "\n",
    "        return train_data, val_data, test_data\n",
    "\n",
    "    return train_data, val_data\n",
    "\n",
    "def parse_record(record, image_only = False, labeled = False, num_classes = 2, sequence = \"t1c\"):\n",
    "\n",
    "    feature_description = {\n",
    "        \"image\": tf.io.FixedLenFeature([240, 240, 3, 4], tf.float32),\n",
    "        \"sex\": tf.io.FixedLenFeature([], tf.int64, default_value=[0]),\n",
    "        \"age\": tf.io.FixedLenFeature([], tf.int64, default_value=0),\n",
    "        \"primary\": tf.io.FixedLenFeature([], tf.int64, default_value=0),\n",
    "    }\n",
    "\n",
    "    example = tf.io.parse_single_example(record, feature_description)\n",
    "    image = example[\"image\"]\n",
    "    image = tf.reshape(image, [240, 240, 3, 4])\n",
    "    #image = data_augmentation(image)\n",
    "\n",
    "    # primary should have a value between 0 and 5\n",
    "    # depending on num classes return different values\n",
    "    # if num_classes = 2, return 1 if primary is 1, else 0\n",
    "    # if num_classes = 3, return primaries 1 and 2, else 0\n",
    "    # if num_classes = 4, return primaries 1, 2 and 3, else 0\n",
    "    # if num_classes = 5, return primaries 1, 2, 3 and 4, else 0\n",
    "    # if num_classes = 6, return primaries 1, 2, 3, 4 and 5, else 0\n",
    "\n",
    "    primary_to_return = tf.constant(0, dtype=tf.int64)\n",
    "\n",
    "    if num_classes == 2:\n",
    "        if example[\"primary\"] == tf.constant(1, dtype=tf.int64):\n",
    "            primary_to_return = example[\"primary\"]\n",
    "        else:\n",
    "            primary_to_return = tf.constant(0, dtype=tf.int64)\n",
    "    elif num_classes == 3:\n",
    "        if example[\"primary\"] == tf.constant(1, dtype=tf.int64) or example[\"primary\"] == tf.constant(2, dtype=tf.int64):\n",
    "            primary_to_return = example[\"primary\"]\n",
    "        else:\n",
    "            primary_to_return = tf.constant(0, dtype=tf.int64)\n",
    "    elif num_classes == 4:\n",
    "        if example[\"primary\"] == tf.constant(1, dtype=tf.int64) or example[\"primary\"] == tf.constant(2, dtype=tf.int64) or example[\"primary\"] == tf.constant(3, dtype=tf.int64):\n",
    "            primary_to_return = example[\"primary\"]\n",
    "        else:\n",
    "            primary_to_return = tf.constant(0, dtype=tf.int64)\n",
    "    elif num_classes == 5:\n",
    "        if example[\"primary\"] == tf.constant(1, dtype=tf.int64) or example[\"primary\"] == tf.constant(2, dtype=tf.int64) or example[\"primary\"] == tf.constant(3, dtype=tf.int64) or example[\"primary\"] == tf.constant(4, dtype=tf.int64):\n",
    "            primary_to_return = example[\"primary\"]\n",
    "        else:\n",
    "            primary_to_return = tf.constant(0, dtype=tf.int64)\n",
    "    elif num_classes == 6:\n",
    "        if example[\"primary\"] == tf.constant(1, dtype=tf.int64) or example[\"primary\"] == tf.constant(2, dtype=tf.int64) or example[\"primary\"] == tf.constant(3, dtype=tf.int64) or example[\"primary\"] == tf.constant(4, dtype=tf.int64) or example[\"primary\"] == tf.constant(5, dtype=tf.int64):\n",
    "            primary_to_return = example[\"primary\"]\n",
    "        else:\n",
    "            primary_to_return = tf.constant(0, dtype=tf.int64)\n",
    "    else:\n",
    "            print(\"ERROR\")\n",
    "            print(\"num classes not supported\")\n",
    "            print(\"Check parse_record function\")\n",
    "            print(\"____________________________\")\n",
    "\n",
    "    \n",
    "    if sequence == \"t1\":\n",
    "        image = image[:, :, :, 0]\n",
    "    elif sequence == \"t1c\":\n",
    "        image = image[:, :, :, 1]\n",
    "    elif sequence == \"t2\":\n",
    "        image = image[:, :, :, 2]\n",
    "    elif sequence == \"flair\":\n",
    "        image = image[:, :, :, 3]\n",
    "\n",
    "    if image_only:\n",
    "        return image, primary_to_return\n",
    "    elif labeled:\n",
    "        return (image, example[\"sex\"], example[\"age\"]), primary_to_return #example[\"primary\"]\n",
    "    else:\n",
    "        return image\n",
    "    \n",
    "def verify_tfrecord(file_path):\n",
    "    try:\n",
    "        for _ in tf.data.TFRecordDataset(file_path, compression_type=\"GZIP\"):\n",
    "            pass\n",
    "    except tf.errors.DataLossError:\n",
    "        print(f\"Corrupted TFRecord file: {file_path}\")\n",
    "\n",
    "def get_callbacks(fold_num = 0,\n",
    "                  use_checkpoint = True,\n",
    "                  use_early_stopping = True,\n",
    "                  early_stopping_patience = early_stopping_patience,\n",
    "                  use_tensorboard = True,\n",
    "                  use_csv_logger = True,\n",
    "                  use_lrscheduler = False):\n",
    "\n",
    "    callbacks = []\n",
    "\n",
    "    path_to_fold_callbacks = path_to_callbacks / f\"fold_{fold_num}\"\n",
    "\n",
    "    def get_run_logdir(root_logdir = path_to_fold_callbacks / \"tensorboard\"):\n",
    "        return Path(root_logdir) / strftime(\"run_%Y_%m_%d_%H_%M_%S\")\n",
    "\n",
    "    run_logdir = get_run_logdir()\n",
    "\n",
    "    # model checkpoint\n",
    "    if use_checkpoint:\n",
    "        checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n",
    "            filepath = path_to_fold_callbacks / \"saved_weights.weights.h5\",\n",
    "            monitor = \"val_accuracy\",\n",
    "            mode = \"max\",\n",
    "            save_best_only = True,\n",
    "            save_weights_only = True,\n",
    "        )\n",
    "        callbacks.append(checkpoint_cb)\n",
    "\n",
    "    # early stopping\n",
    "    if use_early_stopping:\n",
    "        early_stopping_cb = tf.keras.callbacks.EarlyStopping(\n",
    "            patience = early_stopping_patience,\n",
    "            restore_best_weights = True,\n",
    "            verbose = 1\n",
    "        )\n",
    "        callbacks.append(early_stopping_cb)\n",
    "\n",
    "    # tensorboard, doesn't really work yet\n",
    "    if use_tensorboard:\n",
    "        tensorboard_cb = tf.keras.callbacks.TensorBoard(log_dir = run_logdir,\n",
    "                                                    histogram_freq = 1)\n",
    "        callbacks.append(tensorboard_cb)\n",
    "    \n",
    "    # csv logger\n",
    "    if use_csv_logger:\n",
    "        csv_logger_cb = tf.keras.callbacks.CSVLogger(path_to_fold_callbacks / \"training.csv\", separator = \",\", append = True)\n",
    "        callbacks.append(csv_logger_cb)\n",
    "    \n",
    "    if use_lrscheduler:\n",
    "        lr_schedule = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-8 * 10**(epoch * 0.0175))\n",
    "        callbacks.append(lr_schedule)\n",
    "\n",
    "    print(\"get_callbacks successful\")\n",
    "\n",
    "    return callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalizeToRange(tf.keras.layers.Layer):\n",
    "    def __init__(self, zero_to_one=True):\n",
    "        super(NormalizeToRange, self).__init__()\n",
    "        self.zero_to_one = zero_to_one\n",
    "\n",
    "    def call(self, inputs):\n",
    "        min_val = tf.reduce_min(inputs)\n",
    "        max_val = tf.reduce_max(inputs)\n",
    "        if self.zero_to_one:\n",
    "            # Normalize to [0, 1]\n",
    "            normalized = (inputs - min_val) / (max_val - min_val)\n",
    "        else:\n",
    "            # Normalize to [-1, 1]\n",
    "            normalized = 2 * (inputs - min_val) / (max_val - min_val) - 1\n",
    "        return normalized\n",
    "    \n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    tf.keras.layers.RandomFlip(mode = \"horizontal\"),\n",
    "    #tf.keras.layers.Rescaling(1/255),\n",
    "    tf.keras.layers.RandomContrast(0.5), # consider removing the random contrast layer as that causes pixel values to go beyond 1\n",
    "    tf.keras.layers.RandomBrightness(factor = (-0.2, 0.4)), #, value_range=(0, 1)\n",
    "    tf.keras.layers.RandomRotation(factor = (-0.1, 0.1), fill_mode = \"nearest\"),\n",
    "    NormalizeToRange(zero_to_one=True),\n",
    "    tf.keras.layers.RandomTranslation(\n",
    "        height_factor = 0.05,\n",
    "        width_factor = 0.05,\n",
    "        fill_mode = \"nearest\",\n",
    "        interpolation = \"bilinear\"\n",
    "    ),\n",
    "    tf.keras.layers.Resizing(image_size, image_size)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total patients: 276\n",
      "train: 216 | val: 27 | test: 28\n",
      "train: 926 | val: 80 | test: 72\n",
      "get_callbacks successful\n"
     ]
    }
   ],
   "source": [
    "patients = get_patient_paths()\n",
    "\n",
    "train_patients, val_patients, test_patients = split_patients(patients, fraction_to_use = 1)\n",
    "\n",
    "train_paths = get_tfr_paths_for_patients(train_patients)\n",
    "val_paths = get_tfr_paths_for_patients(val_patients)\n",
    "test_paths = get_tfr_paths_for_patients(test_patients)\n",
    "train_data, val_data, test_data = read_data(train_paths, val_paths, test_paths)\n",
    "\n",
    "len_train = len(train_paths)\n",
    "len_val = len(val_paths)\n",
    "len_test = len(test_paths)\n",
    "print(f\"train: {len_train} | val: {len_val} | test: {len_test}\")\n",
    "\n",
    "callbacks = get_callbacks(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "(16, 240, 240, 3)\n",
      "(240, 240, 3)\n",
      "Min: 0.0\n",
      "Max: 255.0\n",
      "Mean: 0.9764930605888367\n",
      "\n",
      "Min: 0.0\n",
      "Max: 1.0\n",
      "Mean: 0.004145030397921801\n",
      "(224, 224, 3)\n",
      "\n",
      "(16,)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_image = train_data.take(1)\n",
    "\n",
    "for image, primary in test_image:\n",
    "    print(primary.numpy()[0])\n",
    "    print(image.shape)\n",
    "    print(image[0].shape)\n",
    "    image = image[0]\n",
    "    print(f\"Min: {tf.reduce_min(image)}\")\n",
    "    print(f\"Max: {tf.reduce_max(image)}\")\n",
    "    print(f\"Mean: {tf.reduce_mean(image)}\")\n",
    "\n",
    "    image = data_augmentation(image)\n",
    "    print()\n",
    "    print(f\"Min: {tf.reduce_min(image)}\")\n",
    "    print(f\"Max: {tf.reduce_max(image)}\")\n",
    "    print(f\"Mean: {tf.reduce_mean(image)}\")\n",
    "    print(image.shape)\n",
    "\n",
    "    print()\n",
    "    print(primary.numpy().shape)\n",
    "\n",
    "    plt.imshow(image, cmap = \"gray\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/LennartPhilipp/Desktop/Uni/Prowiss/Code/Brain_Mets_Classification/brain_mets_env/lib/python3.11/site-packages/vit_keras/utils.py:81: UserWarning: Resizing position embeddings from 24, 24 to 14, 14\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "vit_model = vit.vit_b16(\n",
    "        image_size = image_size,\n",
    "        activation = 'softmax',\n",
    "        pretrained = True,\n",
    "        include_top = False,\n",
    "        pretrained_top = False,\n",
    "        #classes = 3\n",
    "        )\n",
    "\n",
    "#vit_model.build(input_shape=(image_size, image_size, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Patches(tf.keras.layers.Layer):\n",
    "    def __init__(self, patch_size):\n",
    "        super(Patches, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def call(self, images):\n",
    "        batch_size = tf.shape(images)[0]\n",
    "        patches = tf.image.extract_patches(\n",
    "            images = images,\n",
    "            sizes = [1, self.patch_size, self.patch_size, 1],\n",
    "            strides = [1, self.patch_size, self.patch_size, 1],\n",
    "            rates = [1, 1, 1, 1],\n",
    "            padding = 'VALID',\n",
    "        )\n",
    "        patch_dims = patches.shape[-1]\n",
    "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
    "        return patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vision_transformer\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " sequential_27 (Sequential)  (224, 224, 3)             0         \n",
      "                                                                 \n",
      " vit-b16 (Functional)        (None, 768)               85798656  \n",
      "                                                                 \n",
      " flatten_13 (Flatten)        (None, 768)               0         \n",
      "                                                                 \n",
      " batch_normalization_26 (Ba  (None, 768)               3072      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_52 (Dense)            (None, 128)               98432     \n",
      "                                                                 \n",
      " batch_normalization_27 (Ba  (None, 128)               512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_53 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_54 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_55 (Dense)            (None, 3)                 99        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 85911107 (327.72 MB)\n",
      "Trainable params: 85909315 (327.72 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#input_shape = (224, 224, 3)\n",
    "#inputs = tf.keras.Input(shape = input_shape)\n",
    "\n",
    "model_vit = tf.keras.Sequential([\n",
    "        tf.keras.layers.InputLayer(input_shape = (240, 240, 3)),\n",
    "        data_augmentation,\n",
    "        vit_model,\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dense(128, activation = tf.keras.activations.gelu),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dense(64, activation = tf.keras.activations.gelu),\n",
    "        tf.keras.layers.Dense(32, activation = tf.keras.activations.gelu),\n",
    "        tf.keras.layers.Dense(3, activation = 'softmax')\n",
    "    ],\n",
    "    name = 'vision_transformer')\n",
    "\n",
    "model_vit.build()\n",
    "\n",
    "model_vit.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49/49 [==============================] - 186s 4s/step - loss: 1.1954 - accuracy: 0.3222 - val_loss: 1.5543 - val_accuracy: 0.2365\n",
      "Epoch 2/30\n",
      "49/49 [==============================] - 176s 4s/step - loss: 1.0872 - accuracy: 0.4433 - val_loss: 2.0845 - val_accuracy: 0.2500\n",
      "Epoch 3/30\n",
      "49/49 [==============================] - 180s 4s/step - loss: 1.0691 - accuracy: 0.4845 - val_loss: 1.8574 - val_accuracy: 0.1757\n",
      "Epoch 4/30\n",
      "49/49 [==============================] - 180s 4s/step - loss: 1.0676 - accuracy: 0.4601 - val_loss: 1.7427 - val_accuracy: 0.2838\n",
      "Epoch 5/30\n",
      "34/49 [===================>..........] - ETA: 51s - loss: 1.0236 - accuracy: 0.5129"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[353], line 12\u001b[0m\n\u001b[1;32m      5\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer \u001b[38;5;241m=\u001b[39m optimizer, \n\u001b[1;32m      6\u001b[0m               loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse_categorical_crossentropy\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m      7\u001b[0m               metrics \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     10\u001b[0m early_stopping_callbacks \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mEarlyStopping(patience \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m15\u001b[39m, restore_best_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, verbose \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m          \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mval_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m          \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m          \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mearly_stopping_callbacks\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Uni/Prowiss/Code/Brain_Mets_Classification/brain_mets_env/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/Desktop/Uni/Prowiss/Code/Brain_Mets_Classification/brain_mets_env/lib/python3.11/site-packages/keras/src/engine/training.py:1807\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1799\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1800\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1801\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1804\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1805\u001b[0m ):\n\u001b[1;32m   1806\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1807\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1808\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1809\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/Desktop/Uni/Prowiss/Code/Brain_Mets_Classification/brain_mets_env/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/Desktop/Uni/Prowiss/Code/Brain_Mets_Classification/brain_mets_env/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:832\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    829\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    831\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 832\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    834\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    835\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/Desktop/Uni/Prowiss/Code/Brain_Mets_Classification/brain_mets_env/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:868\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    865\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    866\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    867\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 868\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_config\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    872\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    873\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    874\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/Desktop/Uni/Prowiss/Code/Brain_Mets_Classification/brain_mets_env/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Uni/Prowiss/Code/Brain_Mets_Classification/brain_mets_env/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1323\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1319\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1321\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1322\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1323\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1325\u001b[0m     args,\n\u001b[1;32m   1326\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1327\u001b[0m     executing_eagerly)\n\u001b[1;32m   1328\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/Desktop/Uni/Prowiss/Code/Brain_Mets_Classification/brain_mets_env/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m~/Desktop/Uni/Prowiss/Code/Brain_Mets_Classification/brain_mets_env/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/Uni/Prowiss/Code/Brain_Mets_Classification/brain_mets_env/lib/python3.11/site-packages/tensorflow/python/eager/context.py:1486\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1484\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1486\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1487\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1488\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1489\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1490\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1491\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1492\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1493\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1494\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1495\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1496\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1500\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1501\u001b[0m   )\n",
      "File \u001b[0;32m~/Desktop/Uni/Prowiss/Code/Brain_Mets_Classification/brain_mets_env/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-4\n",
    "\n",
    "optimizer = tf.optimizers.legacy.Adam(learning_rate = learning_rate)\n",
    "\n",
    "model_vit.compile(optimizer = optimizer, \n",
    "              loss = \"sparse_categorical_crossentropy\", \n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "\n",
    "early_stopping_callbacks = tf.keras.callbacks.EarlyStopping(patience = 15, restore_best_weights = True, verbose = 1)\n",
    "\n",
    "model_vit.fit(x = train_data,\n",
    "          validation_data = val_data,\n",
    "          epochs = EPOCHS,\n",
    "          callbacks = early_stopping_callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet50V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50v2_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "94668760/94668760 [==============================] - 6s 0us/step\n"
     ]
    }
   ],
   "source": [
    "resnet = tf.keras.applications.ResNet50V2(\n",
    "    include_top=False,\n",
    "    input_shape=(224,224,3),\n",
    "    pooling='avg',\n",
    "    weights='imagenet'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"resnet\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " sequential_28 (Sequential)  (None, 224, 224, 3)       0         \n",
      "                                                                 \n",
      " resnet50v2 (Functional)     (None, 2048)              23564800  \n",
      "                                                                 \n",
      " flatten_14 (Flatten)        (None, 2048)              0         \n",
      "                                                                 \n",
      " batch_normalization_28 (Ba  (None, 2048)              8192      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_56 (Dense)            (None, 128)               262272    \n",
      "                                                                 \n",
      " batch_normalization_29 (Ba  (None, 128)               512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense_57 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_58 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_59 (Dense)            (None, 3)                 99        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 23846211 (90.97 MB)\n",
      "Trainable params: 23796419 (90.78 MB)\n",
      "Non-trainable params: 49792 (194.50 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_resnet = tf.keras.Sequential([\n",
    "    tf.keras.layers.InputLayer(input_shape = (240, 240, 3)),\n",
    "    data_augmentation,\n",
    "    resnet,\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(128, activation = tf.keras.activations.gelu),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "    tf.keras.layers.Dense(64, activation = tf.keras.activations.gelu),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "    tf.keras.layers.Dense(32, activation = tf.keras.activations.gelu),\n",
    "    tf.keras.layers.Dense(3, activation = 'softmax')\n",
    "], name = \"resnet\")\n",
    "\n",
    "model_resnet.build()\n",
    "\n",
    "model_resnet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "58/58 [==============================] - 61s 997ms/step - loss: 1.4287 - accuracy: 0.3326 - val_loss: 1.1051 - val_accuracy: 0.3125\n",
      "Epoch 2/30\n",
      "58/58 [==============================] - 56s 965ms/step - loss: 1.3176 - accuracy: 0.3769 - val_loss: 1.0731 - val_accuracy: 0.4250\n",
      "Epoch 3/30\n",
      "58/58 [==============================] - 58s 994ms/step - loss: 1.2791 - accuracy: 0.3855 - val_loss: 1.0531 - val_accuracy: 0.6125\n",
      "Epoch 4/30\n",
      "58/58 [==============================] - 57s 980ms/step - loss: 1.3141 - accuracy: 0.3585 - val_loss: 0.9283 - val_accuracy: 0.6250\n",
      "Epoch 5/30\n",
      "58/58 [==============================] - 57s 974ms/step - loss: 1.2650 - accuracy: 0.3629 - val_loss: 1.0888 - val_accuracy: 0.3375\n",
      "Epoch 6/30\n",
      "58/58 [==============================] - 56s 970ms/step - loss: 1.2817 - accuracy: 0.3488 - val_loss: 1.0709 - val_accuracy: 0.4125\n",
      "Epoch 7/30\n",
      " 8/58 [===>..........................] - ETA: 49s - loss: 1.1981 - accuracy: 0.3516"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[361], line 12\u001b[0m\n\u001b[1;32m      5\u001b[0m model_resnet\u001b[38;5;241m.\u001b[39mcompile(optimizer \u001b[38;5;241m=\u001b[39m optimizer, \n\u001b[1;32m      6\u001b[0m               loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse_categorical_crossentropy\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m      7\u001b[0m               metrics \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     10\u001b[0m early_stopping_callbacks \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mEarlyStopping(patience \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m15\u001b[39m, restore_best_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, verbose \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m \u001b[43mmodel_resnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m          \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mval_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m          \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m          \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mearly_stopping_callbacks\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Uni/Prowiss/Code/Brain_Mets_Classification/brain_mets_env/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/Desktop/Uni/Prowiss/Code/Brain_Mets_Classification/brain_mets_env/lib/python3.11/site-packages/keras/src/engine/training.py:1807\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1799\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1800\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1801\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1804\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1805\u001b[0m ):\n\u001b[1;32m   1806\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1807\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1808\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1809\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/Desktop/Uni/Prowiss/Code/Brain_Mets_Classification/brain_mets_env/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/Desktop/Uni/Prowiss/Code/Brain_Mets_Classification/brain_mets_env/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:832\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    829\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    831\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 832\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    834\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    835\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/Desktop/Uni/Prowiss/Code/Brain_Mets_Classification/brain_mets_env/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:868\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    865\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    866\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    867\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 868\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_config\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    872\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    873\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    874\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/Desktop/Uni/Prowiss/Code/Brain_Mets_Classification/brain_mets_env/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Uni/Prowiss/Code/Brain_Mets_Classification/brain_mets_env/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1323\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1319\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1321\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1322\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1323\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1325\u001b[0m     args,\n\u001b[1;32m   1326\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1327\u001b[0m     executing_eagerly)\n\u001b[1;32m   1328\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/Desktop/Uni/Prowiss/Code/Brain_Mets_Classification/brain_mets_env/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m~/Desktop/Uni/Prowiss/Code/Brain_Mets_Classification/brain_mets_env/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/Uni/Prowiss/Code/Brain_Mets_Classification/brain_mets_env/lib/python3.11/site-packages/tensorflow/python/eager/context.py:1486\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1484\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1486\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1487\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1488\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1489\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1490\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1491\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1492\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1493\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1494\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1495\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1496\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1500\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1501\u001b[0m   )\n",
      "File \u001b[0;32m~/Desktop/Uni/Prowiss/Code/Brain_Mets_Classification/brain_mets_env/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-4\n",
    "\n",
    "optimizer = tf.optimizers.legacy.Adam(learning_rate = learning_rate)\n",
    "\n",
    "model_resnet.compile(optimizer = optimizer, \n",
    "              loss = \"sparse_categorical_crossentropy\", \n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "\n",
    "early_stopping_callbacks = tf.keras.callbacks.EarlyStopping(patience = 15, restore_best_weights = True, verbose = 1)\n",
    "\n",
    "model_resnet.fit(x = train_data,\n",
    "          validation_data = val_data,\n",
    "          epochs = EPOCHS,\n",
    "          callbacks = early_stopping_callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretrained models\n",
    "https://www.kaggle.com/models/tensorflow/inception \\\n",
    "https://www.kaggle.com/models/google/inception-v3 \\\n",
    "https://www.kaggle.com/models/google/bit \\\n",
    "https://www.kaggle.com/models/google/efficientnet-v2 \\\n",
    "https://www.kaggle.com/models/spsayakpaul/vision-transformer \\\n",
    "https://www.kaggle.com/models/spsayakpaul/convnext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "brain_mets_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
